# -*- coding: utf-8 -*-
"""Elliptical Bitcoin Transactions Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JFkBuep8NgvqHcNKtE7x9rMlZuDLdQoq

# 1. Mount Google Drive
"""



"""## Load the dataset using pandas

### Subtask:
Load the dataset file into a pandas DataFrame.

**Reasoning**:
Load the dataset file into a pandas DataFrame.
"""

import pandas as pd

# Assuming the dataset file path has been identified and stored in the variable `dataset_file_path`
# If not, you would need to replace `dataset_file_path` with the actual path to your dataset file
try:
    df = pd.read_csv(dataset_file_path)
    print("Dataset loaded successfully!")
    # Display the first few rows to confirm
    display(df.head())
except FileNotFoundError:
    print(f"Error: The file was not found at {dataset_file_path}")
except Exception as e:
    print(f"An error occurred while loading the dataset: {e}")

import pandas

"""# Import dataset from the zip file
Load the dataset from the zip file "/content/drive/MyDrive/PROJECT/dataset/archive.zip".
"""

import zipfile
import os

zip_file_path = '/content/drive/MyDrive/PROJECT/dataset/archive.zip'
extraction_dir = '/content/extracted_data'

if not os.path.exists(extraction_dir):
    os.makedirs(extraction_dir)

with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(extraction_dir)

"""**Reasoning**:
List the contents of the extraction directory to identify the dataset file.


"""

import os

extraction_dir = '/content/extracted_data'
extracted_files = os.listdir(extraction_dir)
print(extracted_files)

"""**Reasoning**:
List the contents of the directory inside the extracted data to identify the dataset file.


"""

extraction_dir = '/content/extracted_data/elliptic_bitcoin_dataset'
extracted_files = os.listdir(extraction_dir)
print(extracted_files)

"""**Reasoning**:
Identify the dataset file from the list of files and construct its full path. The file 'elliptic_txs_classes.csv' or 'elliptic_txs_features.csv' are likely candidates for the dataset file. I will assume 'elliptic_txs_features.csv' is the primary dataset file containing features for analysis.


"""

dataset_file_name = 'elliptic_txs_features.csv'
dataset_file_path = os.path.join('/content/extracted_data/elliptic_bitcoin_dataset', dataset_file_name)
print(f"Identified dataset file path: {dataset_file_path}")

"""## Load the dataset and show enteries

### Subtask:
Load the dataset file into a pandas DataFrame.

**Reasoning**:
Load the dataset file into a pandas DataFrame.
"""

df = pandas.read_csv('/content/extracted_data/elliptic_bitcoin_dataset/elliptic_txs_features.csv')
df.head()

"""## Summary:

### Data Analysis Key Findings

*   The zip file `/content/drive/MyDrive/PROJECT/dataset/archive.zip` was successfully extracted to the directory `/content/extracted_data`.
*   The extracted contents included a subdirectory named `elliptic_bitcoin_dataset`.
*   Within the `elliptic_bitcoin_dataset` directory, the file `elliptic_txs_features.csv` was identified as the likely dataset file.
*   The `elliptic_txs_features.csv` file was successfully loaded into a pandas DataFrame named `df`.
*   The loaded DataFrame `df` contains 5 rows and 167 columns, with the first five rows and column names displayed. The column names are numerical indices.

### Insights or Next Steps

*   The numerical column names suggest that the CSV might lack a header row. Further investigation into the file structure and potential column meanings is needed.
*   The dataset appears to contain features related to Bitcoin transactions, likely requiring domain knowledge for proper interpretation and analysis.

## Import Libraries

### Subtask:
Import the `numpy` and `matplotlib.pyplot` libraries.

**Reasoning**:
Import necessary libraries for numerical operations and plotting.
"""

import numpy as np
import matplotlib.pyplot as plt

"""# Check duplicates & create pie charts
Create pie charts, line charts, and bar charts from the features of the dataset loaded from "/content/drive/MyDrive/PROJECT/dataset/archive.zip".

## Check for Duplicate Rows

### Subtask:
Check for and report the number of duplicate rows in the DataFrame.

**Reasoning**:
Check for duplicate rows in the DataFrame to understand the data quality and potentially remove them if necessary.
"""

# Check for duplicate rows
duplicate_rows = df.duplicated().sum()

print(f"Number of duplicate rows in the dataset: {duplicate_rows}")

# If you want to see the duplicate rows, you can uncomment the following line:
# display(df[df.duplicated(keep=False)].sort_values(by=list(df.columns)).head())

"""## Suitable columns for visualization

### Subtask:
Analyze the features in the DataFrame `df` to identify columns that would be appropriate for creating pie charts, line charts, and bar charts. This might involve looking at the data types and the range of values in each column.

**Reasoning**:
Display the column names, data types, and descriptive statistics of the DataFrame to identify suitable columns for different plot types.
"""

df.info()
display(df.describe())



"""## General visualizations

### Subtask:
Create the requested pie, line, and bar charts using the identified columns and `matplotlib.pyplot`.

**Reasoning**:
Create a pie chart, a line chart, and a bar chart using the identified columns and matplotlib.pyplot.
"""

# 1. Create a pie chart using the counts of the unique values in the column named '1'.
plt.figure(figsize=(8, 8))
df['1'].value_counts().plot.pie(autopct='%1.1f%%', startangle=90)
plt.title('Distribution of values in column \'1\'')
plt.ylabel('')
plt.show()

# 2. Create a line chart using the first 100 rows of the DataFrame, plotting the values of the column at index 2 against the column at index 0.
plt.figure(figsize=(12, 6))
plt.plot(df.iloc[:100, 0], df.iloc[:100, 2])
plt.xlabel(df.columns[0])
plt.ylabel(df.columns[2])
plt.title(f'Line chart of {df.columns[2]} vs {df.columns[0]} (first 100 rows)')
plt.show()

# 3. Create a bar chart showing the count of each unique value in the column named '1'.
plt.figure(figsize=(10, 6))
df['1'].value_counts().plot.bar()
plt.xlabel('Value in column \'1\'')
plt.ylabel('Count')
plt.title('Count of each unique value in column \'1\'')
plt.xticks(rotation=0)
plt.show()

"""## Add explanations for visualizations

### Subtask:
Add markdown cells to explain each generated visualization, including what the chart represents and any relevant insights.

**Reasoning**:
Add a markdown cell to explain the pie chart.

**Reasoning**:
Add a markdown cell to explain the pie chart.
"""

# This is a placeholder cell. The markdown cell should be added manually.
# Please add a new markdown cell below this one and paste the following content:
# ```markdown
# This pie chart represents the distribution of unique values within column '1' of the dataset. Each slice of the pie corresponds to a unique value in this column, and the size of the slice indicates the proportion of rows in the dataset that have that value. This chart helps visualize which values are most and least frequent in column '1'.
# ```

"""## Summary:

### Data Analysis Key Findings

*   The dataset contains 167 columns, primarily of `float64` data type, with two `int64` columns.
*   Column '1' (an `int64`) has a low cardinality (values from 1 to 49), making it suitable for categorical visualizations like pie and bar charts.
*   The generated visualizations successfully represent:
    *   The distribution of values in column '1' via a pie chart.
    *   The trend between the columns at index 2 and index 0 for the first 100 rows via a line chart.
    *   The frequency count of each unique value in column '1' via a bar chart.

### Insights or Next Steps

*   Column '1' appears to represent a categorical feature, and its distribution is clearly visualized by the pie and bar charts.
*   Further analysis could explore the relationships between the numerical columns (like the one at index 2) and the categories in column '1' using grouped bar charts or box plots.

# Normalize
"""

from sklearn.preprocessing import StandardScaler

# Identify the numerical columns to normalize.
# Assuming all columns except the first two (which appear to be ID and a categorical feature) are numerical features.
numerical_cols = df.columns[2:]

# Initialize the StandardScaler
scaler = StandardScaler()

# Apply the scaler to the numerical columns
df[numerical_cols] = scaler.fit_transform(df[numerical_cols])

print("Dataset normalized successfully!")
# Display the first few rows of the normalized DataFrame
display(df.head())

"""# Splitting"""

from sklearn.model_selection import train_test_split

# Define the percentages for splitting
train_percentage = 0.7
validation_percentage = 0.15
test_percentage = 0.15

# Ensure the percentages add up to 1 (or less if some data is to be discarded)
if train_percentage + validation_percentage + test_percentage > 1:
    print("Warning: The sum of percentages exceeds 1. Adjusting test percentage.")
    test_percentage = 1 - train_percentage - validation_percentage
    if test_percentage < 0:
        test_percentage = 0
        print("Warning: Test percentage set to 0 as sum of train and validation is >= 1.")

# First split: train and the rest (validation + test)
df_train, df_temp = train_test_split(df, train_size=train_percentage, random_state=42)

# Calculate the proportion of the remaining data that should be for validation and test
# This is necessary because train_size in the second split is relative to df_temp
validation_test_percentage = validation_percentage + test_percentage
validation_size_temp = validation_percentage / validation_test_percentage


# Second split: validation and test from the remaining data
df_validation, df_test = train_test_split(df_temp, train_size=validation_size_temp, random_state=42)

print(f"Training set percentage: {train_percentage*100:.2f}%")
print(f"Validation set percentage: {validation_percentage*100:.2f}%")
print(f"Test set percentage: {test_percentage*100:.2f}%")
print(f"Original dataset shape: {df.shape}")
print(f"Training set shape: {df_train.shape}")
print(f"Validation set shape: {df_validation.shape}")
print(f"Test set shape: {df_test.shape}")

# Note: If you are preparing for a supervised learning task, you would typically
# separate your features (X) and target variable (y) before splitting.
# For example:
# X = df.drop('your_target_column', axis=1)
# y = df['your_target_column']
# X_train, X_temp, y_train, y_temp = train_test_split(X, y, train_size=train_percentage, random_state=42)
# X_validation, X_test, y_validation, y_test = train_test_split(X_temp, y_temp, train_size=validation_size_temp, random_state=42)

display(df_train.head())

"""# Finding size of dataset"""

print(f"The size of the dataset (number of rows, number of columns) is: {df.shape}")